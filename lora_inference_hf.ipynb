{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13dc2ea8-0ce9-47b8-9c9b-556a88330efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "from typing import Optional\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c4562c-8903-493d-b158-be1a5a6ac9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(input_text: str, instruction: Optional[str] = None) -> str:\n",
    "    text = f\"### Question: {input_text}\\n\\n### Answer: \"\n",
    "    if instruction:\n",
    "        text = f\"### Instruction: {instruction}\\n\\n{text}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e178384-7142-4c61-948b-6d66b75b1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token = os.environ.get('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a769066a-79d1-469f-87ba-419e46503eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7125f3c7f7014c188b461904c0ad86fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat.dikshit/Documents/venvs/develop311/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", token=huggingface_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", token=huggingface_token)\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"vdpappu/lora_psychology-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45a45f03-fe33-40c9-9654-daa5705cc560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/venkat.dikshit/Documents/venvs/develop311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "eos_token = '<eos>'\n",
    "eos_token_id = tokenizer.encode(eos_token, add_special_tokens=False)[-1]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "       eos_token_id=tokenizer.eos_token_id,\n",
    "       min_length=5,\n",
    "       max_length=200,\n",
    "       do_sample=True,\n",
    "       temperature=0.7,\n",
    "       top_p=0.9,\n",
    "       top_k=50,\n",
    "       repetition_penalty=1.5,\n",
    "       no_repeat_ngram_size=3,\n",
    "       early_stopping=True\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafa1ccb-c902-4972-a2d5-98b92ccd4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd95061-8267-49b0-9798-db375b2c10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I like the toy but my brother is not sharing it with me. What do I do?\"\n",
    "prompt = generate_prompt(input_text=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61ef938-5449-4fb9-9d14-ce280d481cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = merged_model.generate(**inputs, generation_config=generation_config)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afef1f17-b7b6-4e5a-baed-4b897b741cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 27.31 seconds\n",
      "### Question: I like the toy but my brother is not sharing it with me. What do I do?\n",
      "\n",
      "### Answer: </h6>\n",
      "<h6>Don't be a crybaby and just take it from your brother, he probably doesn't want you to play with it anyway! It might even make him feel guilty if they give in easily for one of their own children or friends that don't have anything else yet anyways...</h6>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inference time: {end-start:.2f} seconds\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd802398-5b74-4778-886e-28b42362080e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (develop311)",
   "language": "python",
   "name": "develop311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
